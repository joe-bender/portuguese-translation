{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portuguese to English Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I load the filtered dataset from the previous notebook and then train an encoder-decoder model to translate short Portuguese phrases to English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I import the libraries that I need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set the random seeds for PyTorch, NumPy and Python's `random` library so others can get the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5)\n",
    "np.random.seed(3)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load in the pickled DataFrame from the previous notebook, which is already in the correct format for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('short_df.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data into training and validation sets, I first shuffle the rows of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "df = df.sample(frac=1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45812"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I choose a split point that will put 90% of the items in the training set and 10% in the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41230"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut = int(len(df)*.9)\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:cut]\n",
    "df_val = df[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41230, 4582)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to tokenize the Potuguese text separately from the English text, since they clearly have distinct vocabularies (except for some overlap, like proper nouns). First I create full flat lists of all tokens in each set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_alltoks = []\n",
    "for sent in df_train['pt_toks']:\n",
    "    for tok in sent:\n",
    "        pt_alltoks.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_alltoks = []\n",
    "for sent in df_train['en_toks']:\n",
    "    for tok in sent:\n",
    "        en_alltoks.append(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I create the vocab lists by sorting the tokens in order of most common (although this isn't really necessary, but it's nice for inspection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14872, 10715)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_vocab = [tok for tok, count in Counter(pt_alltoks).most_common()]\n",
    "en_vocab = [tok for tok, count in Counter(en_alltoks).most_common()]\n",
    "len(pt_vocab), len(en_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I add special tokens to each vocab list:\n",
    "- UNK is for unknown tokens (which may appear in the validation set but not in the training set)\n",
    "- PAD is for padding the ends of sequences which are not as long as others in the same batch\n",
    "- START marks the start of each sentence\n",
    "- END marks the end of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_toks = ['UNK', 'PAD', 'START', 'END']\n",
    "pt_vocab = special_toks + pt_vocab\n",
    "en_vocab = special_toks + en_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I make a tokenization method, which will extract all tokens from each string by matching on words or punctuation, without combining them (so avoiding tokens that include a word and a punctuation mark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(sent):\n",
    "    return re.findall(r'\\w+|[^\\w\\s]+', sent) # matches words or punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I convert tokens into integers for use in the neural network by assigning each token a number based on its index in its vocab list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tok2num = {tok:num for num,tok in enumerate(pt_vocab)}\n",
    "pt_num2tok = {num:tok for num,tok in enumerate(pt_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok2num = {tok:num for num,tok in enumerate(en_vocab)}\n",
    "en_num2tok = {num:tok for num,tok in enumerate(en_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I make functions that can deal with out-of-dictionary tokens by replacing them with the UNK token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_numericalize(tok):\n",
    "    return pt_tok2num[tok] if tok in pt_tok2num.keys() else pt_tok2num['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_numericalize(tok):\n",
    "    return en_tok2num[tok] if tok in en_tok2num.keys() else en_tok2num['UNK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make encode and decode functions that add START and END tokens to each sentence and also convert them to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_encode(sent):\n",
    "    toks = ['START'] + tokenize_sentence(sent) + ['END']\n",
    "    nums = [pt_numericalize(tok) for tok in toks]\n",
    "    tens = torch.tensor(nums)\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's decoder will add the START token to English sentences, so I only need to add the END token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_encode(sent):\n",
    "    toks = tokenize_sentence(sent) + ['END']\n",
    "    nums = [en_numericalize(tok) for tok in toks]\n",
    "    tens = torch.tensor(nums)\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decode functions do the opposite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_decode(tens):\n",
    "    nums = tens.numpy().tolist()\n",
    "    nums = nums[1:-1] # remove START and END tokens\n",
    "    toks = [pt_num2tok[num] for num in nums]\n",
    "    sentence = ' '.join(toks)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_decode(tens):\n",
    "    nums = tens.numpy().tolist()\n",
    "    nums = nums[:-1] # remove END\n",
    "    toks = [en_num2tok[num] for num in nums]\n",
    "    sentence = ' '.join(toks)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Don't confuse these encode and decode functions with the encoder and decoder in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create batches by first creating a batch as a small DataFrame, then extracting x and y tensor batches from that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I choose a batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I create a function to extract the input tensor batch from a DataFrame batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xb_from_dfb(dfb):\n",
    "    tens_list = [pt_encode(o) for o in dfb['pt']]\n",
    "    max_len = dfb['pt_len'].max() + 2 # start and end tokens also\n",
    "    padded = torch.ones(bs,max_len).long()\n",
    "    for i,t in enumerate(tens_list):\n",
    "        padded[i,:len(t)] = t\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find the longest sentence in the whole dataset and store it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_en_len = df['en_len'].max()\n",
    "max_en_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I create a function to extract the output tensor from a DataFrame batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yb_from_dfb(dfb):\n",
    "    tens_list = [en_encode(o) for o in dfb['en']]\n",
    "    max_len = max_en_len + 1 # end token also\n",
    "    padded = torch.ones(bs,max_len).long()\n",
    "    for i,t in enumerate(tens_list):\n",
    "        padded[i,:len(t)] = t\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I create a generator to create a DataFrame batch and then use the previous functions to output the x and y batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(df):\n",
    "    random_state = random.randint(0,100) # I need a random shuffle each time\n",
    "    df_shuf = df.sample(frac=1, random_state=random_state) # the random state is different each time\n",
    "    for i in range(0,len(df),bs):\n",
    "        start = i\n",
    "        end = i+bs if i+bs < len(df) else len(df)\n",
    "        dfb = df_shuf.iloc[start:end]\n",
    "        yield xb_from_dfb(dfb), yb_from_dfb(dfb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create my encoder-decoder model, I first need to choose the sizes of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder embedding layer will go from the size of the Portuguese vocab to the embedding size, so I need the vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14876"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_voc_sz = len(pt_vocab)\n",
    "pt_voc_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need the English vocab size for the decoder's embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10719"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_voc_sz = len(en_vocab)\n",
    "en_voc_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'll use the same embedding size for both encoder and decoder, and also for the LSTM's hidden size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates the START token for a full batch in the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_start(bs):\n",
    "    return torch.ones(bs,1).long()*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I create the full encoder-decoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It includes these layers:\n",
    "- an Embedding layer for the encoder\n",
    "- and LSTM layer for the encoder\n",
    "- an Embedding layer for the decoder\n",
    "- and LSTM layer for the decoder\n",
    "- a Linear layer for the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method uses the encoder to create vector representations of each sentence in the batch, which are then passed as LSTM hidden state to the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder works a lot like a normal LSTM language model, but it also starts with the vector representations as hidden state from the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder uses a technique called \"teacher forcing\", which uses the true input of the English sentence at each sequence step to predict the following token. If I didn't use this technique, the decoder would have to base most of its predictions on incorrect previous predictions most of the time at the start of training, which makes the task much harder. It effectively makes the task a lot more like a normal language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My slight adjustment to this technique is that I add a hyperparameter `p` to the forward method. At every decoder time step, I use the true previous token with probability `p` (normal \"teacher forcing\"), but with probability `1-p` I use the predicted previous token. My idea is that this will make the model more flexible and stronger, since this is closer to the inference prediction task (which doesn't get to see any of the y data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method can be used for inference by simply not supplying `y` or `p` arguments. When `y` is not given, only previous predictions will be used as input in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Portuguese encoder\n",
    "        self.pt_emb = nn.Embedding(pt_voc_sz, emb_sz, padding_idx=1)\n",
    "        self.pt_lstm = nn.LSTM(emb_sz, emb_sz, 1, batch_first=True)\n",
    "        # English decoder\n",
    "        self.en_emb = nn.Embedding(en_voc_sz, emb_sz, padding_idx=1)\n",
    "        self.en_lstm = nn.LSTM(emb_sz, emb_sz, 1, batch_first=True)\n",
    "        self.en_lin = nn.Linear(emb_sz, en_voc_sz)\n",
    "        self.en_lin.weight = self.en_emb.weight # weight tying\n",
    "    \n",
    "    # using teacher forcing\n",
    "    def forward(self, x, y=None, p=.9):\n",
    "        # encoder\n",
    "        _, h = self.pt_lstm(self.pt_emb(x))\n",
    "        # decoder\n",
    "        en_in = en_start(len(x)) # START token\n",
    "        outputs = []\n",
    "        for i in range(max_en_len+1): # add 1 for STOP token\n",
    "            if y != None: # y is only included in training\n",
    "                if i>0: # for any sequence step after the first\n",
    "                    # use true previous token with probability p\n",
    "                    if random.random() < p: en_in = y[:,i-1:i]\n",
    "            out, h = self.en_lstm(self.en_emb(en_in), h)\n",
    "            out = self.en_lin(out)\n",
    "            en_in = out.argmax(dim=-1)\n",
    "            outputs.append(out)\n",
    "        return torch.cat(outputs, dim=1) # concat on sequence dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TranslateModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My loss function is the standard language model cross-entropy loss. I just need to flatten the predictions and targets before using the PyTorch function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_loss(preds, targs):\n",
    "    preds = preds.view(-1, en_voc_sz)\n",
    "    targs = targs.view(-1)\n",
    "    return F.cross_entropy(preds, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just use the very effective Adam optimizer since it works well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a standard training function, but with a `p` value supplied to determine the chance of teacher forcing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(p):\n",
    "    for xb, yb in get_batches(df_train):\n",
    "        out = model(xb, yb, p=p)\n",
    "        loss = translate_loss(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also use a standard validation function that returns the mean loss over the full validation set. I don't pass a `y` to the model, so this is the same functionality as I'll use during inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch():\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in get_batches(df_val):\n",
    "            out = model(xb)\n",
    "            loss = translate_loss(out, yb)\n",
    "            losses.append(loss)\n",
    "    return np.array(losses).mean().item()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not using any additional metric (such as accuracy) since I want to check the effectiveness on the validation set at the end of training by manually checking the results. Translation is tricky because there are many ways to \"correctly\" translate a sentence, so I'll use my own judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, I'll use a decaying `p` value for each epoch, starting at 1 (which means no reliance on previous predictions) and ending at .6 (which means a 60% chance at each time step of relying on the previous prediction). This lets the model start with an easier task and increase the difficulty as it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Validation loss: 5.276256084442139\n",
      "Epoch 2 completed. Validation loss: 5.27774715423584\n",
      "Epoch 3 completed. Validation loss: 5.353205680847168\n",
      "Epoch 4 completed. Validation loss: 5.444311141967773\n",
      "Epoch 5 completed. Validation loss: 4.595528602600098\n",
      "Epoch 6 completed. Validation loss: 4.233743667602539\n",
      "Epoch 7 completed. Validation loss: 4.03050422668457\n",
      "Epoch 8 completed. Validation loss: 3.769270181655884\n"
     ]
    }
   ],
   "source": [
    "ps = [1., 1., 1., 1., .9, .8, .7, .6]\n",
    "for i,p in enumerate(ps):\n",
    "    train_epoch(p=p)\n",
    "    val_loss = validate_epoch()\n",
    "    print(f'Epoch {i+1} completed. Validation loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get a batch of inputs from the validation set and pass them through the model in inference mode (by not passing in any `y` data). I then group the predicted sentences with the true label sentences and the input sentences for further examination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xb, yb = next(get_batches(df_val))\n",
    "out = model(xb)\n",
    "nums = out.argmax(dim=-1)\n",
    "\n",
    "ins = [pt_decode(o) for o in xb]\n",
    "trues = [en_decode(o) for o in yb]\n",
    "preds = [en_decode(o) for o in nums]\n",
    "\n",
    "examples = list(zip(ins, trues, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I compare inputs and true labels to predictions to judge how well the model did. Each example features three sentences:\n",
    "- the input Portuguese sentence\n",
    "- the English translation that is the true label from the dataset\n",
    "- the English translation that the model generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of inputs that probably overlap with the training data and have been memorized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('( o parlamento aprova a acta ) END',\n",
       " '( the minutes were approved ) END PAD',\n",
       " '( the minutes were approved ) END PAD')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('( o parlamento aprova a resolução ) END',\n",
       " '( parliament adopted the resolution ) END PAD',\n",
       " '( parliament adopted the resolution ) END PAD')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a votação terá lugar hoje . END PAD',\n",
       " 'the vote will take place today . END',\n",
       " 'the vote will take place today . END')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memorization is not ideal, since it only works on data that has been seen in the training set. However, it can still be useful for phrases that are so common that they might as well be memorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more interesting results have translations that are not exactly the same as the true labels but still are pretty accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, the model's output is actually a more literal translation of the Portuguese sentence, which could be literally translated as something like \"I don't have any doubt about that\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('não tenho qualquer dúvida quanto a isso .',\n",
       " 'of this i am in no doubt .',\n",
       " 'i have no doubt about that . END')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example again has a paraphrased version of the true label translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('em consequência , votámos contra este relatório .',\n",
       " 'we have therefore voted against this report .',\n",
       " 'we are now voted against this report .')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next prediction actually makes more sense than the label translation, at least when this sentence is not in any surrounding context. \"precisamos\" means \"we need to\", and \"dar-lhes tempo\" means \"give them time\", so the prediction seems more accurate at least at the beginning of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('precisamos dar - lhes tempo . END PAD',\n",
       " 'it must grant the time for them .',\n",
       " 'we need to do them . END PAD')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these previous translations could again be based on some memorization of different label translations for the same common input sentence, shared between training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting predictions are the ones that are clearly not memorized because they are obviously not how any human would translate the inputs, but they still demonstrate some strange understanding of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one translates \"very clear\" as \"clear clear\", which makes sense in a way, because it is repeating and thus emphasizing the idea of \"clear\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('esta mensagem é muito clara . END PAD',\n",
       " 'that message is very clear . END PAD',\n",
       " 'this message is clear clear . END PAD')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one doesn't quite work grammatically but seems to understand that more positivity is needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('temos de adoptar uma atitude mais positiva .',\n",
       " 'we must adopt a more positive approach .',\n",
       " 'we must be a positive more . END')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one knows about a decision and its location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a decisão desta assembleia está tomada . END',\n",
       " 'the house has made its decision . END',\n",
       " 'the decision is in this house . END')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one shows the model has an idea of the relation between the concepts of priority and importance, although it fumbles the full translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('estas prioridades são de três ordens . END',\n",
       " 'there are three types of priority . END',\n",
       " 'these three three situations are important . END')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one translates the first half of the sentence very well: \"é a única coisa\" by itself would be \"it is the only thing\". It didn't translate the last half that should be \"that matters\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('é a única coisa que importa ! END',\n",
       " 'that is all that counts . END PAD',\n",
       " 'it is the only one ! ! END')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one understood the concept but chose the most obvious word both times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('isto é bom e positivo . END PAD',\n",
       " 'this is both good and positive . END',\n",
       " 'this is good and good . END PAD')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[46]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one knows the need for awareness, but not what we need to be aware of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('temos de estar cientes desse risco . END',\n",
       " 'we must be aware of this risk .',\n",
       " 'we need to be aware of this .')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following translations have more problems but are still interesting to examine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one got too focused on the ideas of \"important\" and \"two\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tomaram - se assim dois passos importantes .',\n",
       " 'two important steps have thus been taken .',\n",
       " 'important two two points important points . END')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[59]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one has an interesting Yoda-like way of saying \"we don't need more\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('não precisamos de mais directivas . END PAD',\n",
       " 'we do not need more directives . END',\n",
       " 'we need more not be . END PAD')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[44]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one has an interesting way of saying something isn't finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('esse trabalho ainda não foi concluído . END',\n",
       " 'this work is on - going . END',\n",
       " 'this has yet been not yet . END')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[53]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, this one thinks Mrs. Pack is so correct that she should be addressed as Mrs. Right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tem razão , senhora deputada pack . END',\n",
       " 'you are right , mrs pack . END',\n",
       " 'you are , mrs right . END PAD')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[47]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
